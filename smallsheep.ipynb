{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Bell Pepper Product Classifier - Optimized for High Recall\n",
        "This notebook builds a classifier to identify bell pepper products with emphasis on minimizing false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_cell"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, fbeta_score, make_scorer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data_header"
      },
      "source": [
        "## 1. Load and explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data_cell"
      },
      "outputs": [],
      "source": [
        "# Download the data from GitHub\n",
        "!wget https://raw.githubusercontent.com/gkwa/smallsheep/master/bell_peppers.json -O bell_peppers.json\n",
        "\n",
        "# Read the JSON file\n",
        "with open('bell_peppers.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# View basic information\n",
        "print(f\"Total number of products: {len(df)}\")\n",
        "print(f\"Number of true bell peppers: {df['is_bell_pepper'].sum()}\")\n",
        "print(f\"Number of non-bell peppers: {len(df) - df['is_bell_pepper'].sum()}\")\n",
        "print(f\"Bell pepper percentage: {df['is_bell_pepper'].mean() * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_eng_header"
      },
      "source": [
        "## 2. Feature Engineering - Enhanced with lessons learned\n",
        "\n",
        "We're creating enhanced features that address specific patterns we've found in our analysis, including:\n",
        "1. Detection of tri-color/multi-color bell peppers that were previously misclassified\n",
        "2. Better identification of processed and artificial products\n",
        "3. Recognition of brand name positioning in product names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_extraction_cell"
      },
      "outputs": [],
      "source": [
        "# Helper functions for feature extraction with improvements\n",
        "def extract_enhanced_features(product_name):\n",
        "    \"\"\"Extract enhanced boolean features from product name\"\"\"\n",
        "    product_name = product_name.lower()\n",
        "    \n",
        "    # Color features\n",
        "    has_red = 'red' in product_name\n",
        "    has_green = 'green' in product_name\n",
        "    has_yellow = 'yellow' in product_name\n",
        "    has_orange = 'orange' in product_name\n",
        "    \n",
        "    # New feature: detect tri-color or multi-color patterns\n",
        "    has_tricolor = any(term in product_name for term in \n",
        "                      ['tri-color', 'tricolor', 'tri color', 'rainbow', 'mixed', 'stoplight'])\n",
        "    \n",
        "    # Negative signal features\n",
        "    is_processed = any(term in product_name for term in \n",
        "                      ['roasted', 'dried', 'dehydrated', 'strips', 'slices', 'diced', 'powder', 'crushed'])\n",
        "    is_artificial = any(term in product_name for term in \n",
        "                      ['artificial', 'fake', 'toy', 'seeds', 'seed', 'hybrid', 'decoration'])\n",
        "    is_jarred = any(term in product_name for term in \n",
        "                   ['jar', 'pack', 'can', 'canned', 'oz', 'ounce'])\n",
        "    \n",
        "    # Positive signal features\n",
        "    is_fresh = 'fresh' in product_name\n",
        "    has_count = any(term in product_name for term in \n",
        "                   ['count', 'each', 'ct'])\n",
        "    is_organic = 'organic' in product_name\n",
        "    \n",
        "    # Position features\n",
        "    starts_with_brand = bool(re.match(r'^[A-Za-z]+Â®|^[A-Za-z]+ brand', product_name))\n",
        "    \n",
        "    # Produce department signals\n",
        "    is_produce = any(term in product_name for term in ['produce', 'fresh'])\n",
        "    \n",
        "    return {\n",
        "        'has_red': has_red,\n",
        "        'has_green': has_green,\n",
        "        'has_yellow': has_yellow,\n",
        "        'has_orange': has_orange,\n",
        "        'has_tricolor': has_tricolor,\n",
        "        'is_processed': is_processed,\n",
        "        'is_artificial': is_artificial,\n",
        "        'is_jarred': is_jarred,\n",
        "        'is_fresh': is_fresh,\n",
        "        'has_count': has_count,\n",
        "        'is_organic': is_organic,\n",
        "        'starts_with_brand': starts_with_brand,\n",
        "        'is_produce': is_produce\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_pipeline_cell"
      },
      "outputs": [],
      "source": [
        "# Create a custom feature extractor for the sklearn pipeline\n",
        "class ProductFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Apply feature extraction to all product names\n",
        "        features_list = [extract_enhanced_features(name) for name in X]\n",
        "        return pd.DataFrame(features_list)\n",
        "\n",
        "# Create TF-IDF features from product names\n",
        "tfidf = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 3),  # Unigrams, bigrams, and trigrams for better phrase capture\n",
        "    max_features=150,    # Expanded for better coverage\n",
        "    token_pattern=r'(?u)\\b\\w+\\b'  # Capture single-character words like \"3\"\n",
        ")\n",
        "\n",
        "# Extract manual features for visualization/analysis\n",
        "manual_features = df['name'].apply(extract_enhanced_features)\n",
        "manual_features_df = pd.DataFrame(manual_features.tolist())\n",
        "\n",
        "# Add these features to the original dataframe for analysis\n",
        "df_with_features = pd.concat([df, manual_features_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eda_header"
      },
      "source": [
        "## 3. Exploratory Data Analysis\n",
        "\n",
        "Let's explore the relationship between our features and the target variable (is_bell_pepper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_cell"
      },
      "outputs": [],
      "source": [
        "# Examine feature correlation with the target\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Only include boolean/numeric columns for correlation analysis\n",
        "numeric_cols = ['is_bell_pepper'] + list(manual_features_df.columns)\n",
        "corr = df_with_features[numeric_cols].corr()\n",
        "sns.heatmap(corr[['is_bell_pepper']], annot=True, cmap='coolwarm')\n",
        "plt.title('Feature Correlation with is_bell_pepper')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_dist_cell"
      },
      "outputs": [],
      "source": [
        "# Visualize feature distributions by label\n",
        "fig, axes = plt.subplots(7, 2, figsize=(15, 28))\n",
        "axes = axes.flatten()\n",
        "\n",
        "feature_cols = manual_features_df.columns\n",
        "for i, feature in enumerate(feature_cols):\n",
        "    if i < len(axes):\n",
        "        sns.countplot(x=feature, hue='is_bell_pepper', data=df_with_features, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {feature}')\n",
        "        axes[i].set_ylabel('Count')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split_header"
      },
      "source": [
        "## 4. Train/Test Split\n",
        "\n",
        "Split the data into training and test sets, ensuring the class distribution is preserved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "split_cell"
      },
      "outputs": [],
      "source": [
        "X = df['name']\n",
        "y = df['is_bell_pepper']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipeline_header"
      },
      "source": [
        "## 5. Build a combined Text + Features Pipeline with Random Forest\n",
        "\n",
        "Create a pipeline that combines TF-IDF features and our custom engineered features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pipeline_cell"
      },
      "outputs": [],
      "source": [
        "# Now using both TF-IDF and custom features through FeatureUnion\n",
        "combined_pipeline = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('tfidf', tfidf),\n",
        "        ('custom', ProductFeatureExtractor())\n",
        "    ])),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optimize_header"
      },
      "source": [
        "## 6. Optimize for high recall with F2 score\n",
        "\n",
        "Use F2 scoring which prioritizes recall over precision during model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grid_search_cell"
      },
      "outputs": [],
      "source": [
        "# F2 score gives more weight to recall than precision (beta=2)\n",
        "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
        "\n",
        "# Hyperparameter tuning with focus on high recall\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 15, 30],\n",
        "    'classifier__min_samples_split': [2, 5],\n",
        "    'classifier__class_weight': [None, 'balanced', {False: 1, True: 5}]  # Add class weights to prioritize bell peppers\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    combined_pipeline, \n",
        "    param_grid, \n",
        "    cv=5, \n",
        "    scoring=f2_scorer,  # Use F2 instead of F1 to prioritize recall\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation F2 score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_default_header"
      },
      "source": [
        "## 7. Evaluate the Model with default threshold (0.5)\n",
        "\n",
        "First, we'll evaluate the model with the standard classification threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_default_cell"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nModel Performance with Default Threshold (0.5):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion_default_cell"
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Not Bell Pepper', 'Bell Pepper'],\n",
        "            yticklabels=['Not Bell Pepper', 'Bell Pepper'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (Default Threshold)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "threshold_header"
      },
      "source": [
        "## 8. Evaluate Model with Lower Threshold for High Recall\n",
        "\n",
        "Try different classification thresholds to find the optimal one for eliminating false negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "threshold_search_cell"
      },
      "outputs": [],
      "source": [
        "# Try multiple thresholds to find the optimal point\n",
        "thresholds = [0.5, 0.3, 0.2, 0.1, 0.05]\n",
        "results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Apply threshold to prediction probabilities\n",
        "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_threshold)\n",
        "    report = classification_report(y_test, y_pred_threshold, output_dict=True)\n",
        "    \n",
        "    # Extract key metrics\n",
        "    results.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': accuracy,\n",
        "        'recall_bell_pepper': report['True']['recall'],\n",
        "        'precision_bell_pepper': report['True']['precision'],\n",
        "        'f1_bell_pepper': report['True']['f1-score'],\n",
        "        'false_negatives': (y_test == True).sum() - ((y_test == True) & (y_pred_threshold == True)).sum()\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for easier comparison\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nPerformance at Different Thresholds:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "best_threshold_cell"
      },
      "outputs": [],
      "source": [
        "# Find the threshold with perfect recall (if any)\n",
        "perfect_recall_thresholds = results_df[results_df['recall_bell_pepper'] == 1.0]\n",
        "if not perfect_recall_thresholds.empty:\n",
        "    best_threshold = perfect_recall_thresholds.iloc[0]['threshold']\n",
        "    print(f\"\\nLowest threshold with perfect recall: {best_threshold}\")\n",
        "else:\n",
        "    # Otherwise choose the threshold with highest recall\n",
        "    best_threshold = results_df.loc[results_df['recall_bell_pepper'].idxmax()]['threshold']\n",
        "    print(f\"\\nThreshold with highest recall: {best_threshold}\")\n",
        "\n",
        "# Use the selected threshold for final evaluation\n",
        "y_pred_high_recall = (best_model.predict_proba(X_test)[:, 1] >= best_threshold).astype(int)\n",
        "\n",
        "print(f\"\\nModel Performance with Optimized Threshold ({best_threshold}):\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_high_recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion_optimized_cell"
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix with optimized threshold\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm_high_recall = confusion_matrix(y_test, y_pred_high_recall)\n",
        "sns.heatmap(cm_high_recall, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Not Bell Pepper', 'Bell Pepper'],\n",
        "            yticklabels=['Not Bell Pepper', 'Bell Pepper'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title(f'Confusion Matrix (Threshold = {best_threshold})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_importance_header"
      },
      "source": [
        "## 9. Feature Importance Analysis\n",
        "\n",
        "Examine which features are most influential in the classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_importance_cell"
      },
      "outputs": [],
      "source": [
        "# Extract feature importance from the Random Forest model\n",
        "if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
        "    # Get feature names\n",
        "    feature_names = []\n",
        "    \n",
        "    # Get names from TF-IDF features\n",
        "    tfidf_feature_names = best_model.named_steps['features'].transformer_list[0][1].get_feature_names_out()\n",
        "    for name in tfidf_feature_names:\n",
        "        feature_names.append(f'tfidf__{name}')\n",
        "    \n",
        "    # Get names from custom features\n",
        "    custom_feature_names = list(extract_enhanced_features('dummy').keys())\n",
        "    for name in custom_feature_names:\n",
        "        feature_names.append(f'custom__{name}')\n",
        "    \n",
        "    # Match feature names with importance\n",
        "    importances = best_model.named_steps['classifier'].feature_importances_\n",
        "    \n",
        "    # If length mismatch, just use indices\n",
        "    if len(feature_names) != len(importances):\n",
        "        feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
        "    \n",
        "    # Create a DataFrame for feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': feature_names[:len(importances)],\n",
        "        'Importance': importances\n",
        "    })\n",
        "    \n",
        "    # Sort by importance\n",
        "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "    \n",
        "    # Plot top 20 features\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "    plt.title('Top 20 Most Important Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "misclassified_header"
      },
      "source": [
        "## 10. Analyze Misclassifications with High-Recall Model\n",
        "\n",
        "Examine which products are still being misclassified with our optimized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "misclassified_cell"
      },
      "outputs": [],
      "source": [
        "misclassified = X_test[y_test != y_pred_high_recall].reset_index(drop=True)\n",
        "misclassified_labels = y_test[y_test != y_pred_high_recall].reset_index(drop=True)\n",
        "misclassified_df = pd.DataFrame({\n",
        "    'product_name': misclassified,\n",
        "    'true_label': misclassified_labels,\n",
        "    'predicted_label': y_pred_high_recall[y_test != y_pred_high_recall]\n",
        "})\n",
        "\n",
        "print(\"\\nMisclassified Products (High-Recall Setting):\")\n",
        "display(misclassified_df)\n",
        "\n",
        "# Convert to markdown table for better readability\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def df_to_markdown(df):\n",
        "    \"\"\"Convert a pandas DataFrame to a markdown table\"\"\"\n",
        "    markdown = \"| \" + \" | \".join(df.columns) + \" |\\n\"\n",
        "    markdown += \"| \" + \" | \".join([\"---\"] * len(df.columns)) + \" |\\n\"\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        values = []\n",
        "        for col in df.columns:\n",
        "            # Format true/false as text and numbers as strings\n",
        "            if isinstance(row[col], bool):\n",
        "                values.append(\"True\" if row[col] else \"False\")\n",
        "            elif isinstance(row[col], (int, float)):\n",
        "                values.append(str(row[col]))\n",
        "            else:\n",
        "                values.append(str(row[col]))\n",
        "        markdown += \"| \" + \" | \".join(values) + \" |\\n\"\n",
        "    \n",
        "    return markdown\n",
        "\n",
        "print(\"\\nMisclassified Products (High-Recall Setting) - Markdown Table:\")\n",
        "display(Markdown(df_to_markdown(misclassified_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "testing_header"
      },
      "source": [
        "## 11. Test with Additional Examples\n",
        "\n",
        "Test the model on both previously challenging products and new examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "testing_function_cell"
      },
      "outputs": [],
      "source": [
        "# Define a function to predict with the optimal threshold\n",
        "def predict_with_threshold(product_name, model, threshold=best_threshold):\n",
        "    \"\"\"Predict if a product is a bell pepper using the optimized threshold\"\"\"\n",
        "    probability = model.predict_proba([product_name])[0][1]\n",
        "    prediction = probability >= threshold\n",
        "    \n",
        "    print(f\"Product: {product_name}\")\n",
        "    print(f\"Prediction: {'Bell Pepper' if prediction else 'Not Bell Pepper'}\")\n",
        "    print(f\"Confidence of being a Bell Pepper: {probability:.2f}\")\n",
        "    print(f\"Threshold: {threshold}\")\n",
        "    return prediction, probability\n",
        "\n",
        "# Test examples\n",
        "test_examples = [\n",
        "    \"Organic Red Bell Pepper\",\n",
        "    \"Fresh Yellow Bell Pepper, Each\",\n",
        "    \"KrogerÂ® Tri-Color Bell Peppers\",  # Previously misclassified\n",
        "    \"Mezzetta Roasted Red Bell Peppers\",\n",
        "    \"Mini Sweet Peppers\",\n",
        "    \"Bell Pepper Seeds for Planting\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting model on example products:\")\n",
        "for example in test_examples:\n",
        "    predict_with_threshold(example, best_model)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "additional_testing_cell"
      },
      "outputs": [],
      "source": [
        "# Additional test subjects from newest data\n",
        "additional_test_subjects = [\n",
        "    \"Burpee 'Sweet Candy Apple' Hybrid | Bell Pepper\",\n",
        "    \"Dried Red and Green Bell Peppers Mix by It's Delish, 1 lb Bulk Bag, All Natural\",\n",
        "    \"Dried Red and Green Bell Peppers Mix by It\",\n",













		\"Dried Red and Green Bell Peppers Mix by It's Delish, 9 oz Large Jar, Chopped, Spice Seasoning\",\n",
    \"GranAroma Red Bell Pepper Powder, Sweet Pepper Flavor, Baking & Cooking (17 Ounce)\",\n",
    \"Green BELL Peppers 1 Pound Bulk Bag-Heat Sealed to Maintain Freshness-Crushed & Dried Spice Seasoning\",\n",
    \"Holiday Wedding Party Decoration Mini Bell Peppers Vegetables For Decoration Pepper Decoration 2/3/4PCS\",\n",
    \"Howard Foods Howards Sweet Pepper Relish, 11 oz Jar\",\n",
    \"Mezzetta Sweet Banana Peppers, 16 fl oz Jar\",\n",
    \"TAZAH Chili Peppers 22oz (624g) â Pickled Hot Yellow Cascabella Peppers\"\n",
]\n",
"\n",
"# Create a dataframe to store results for additional test subjects\n",
"additional_results_df = pd.DataFrame(columns=['Product', 'Prediction', 'Confidence'])\n",
"\n",
"print(\"\\nTesting model on additional product examples:\")\n",
"for example in additional_test_subjects:\n",
"    prediction, probability = predict_with_threshold(example, best_model)\n",
"    print(\"-\" * 50)\n",
"    \n",
"    # Add to results dataframe\n",
"    additional_results_df = pd.concat([additional_results_df, pd.DataFrame({\n",
"        'Product': [example],\n",
"        'Prediction': ['Bell Pepper' if prediction else 'Not Bell Pepper'],\n",
"        'Confidence': [probability]\n",
"    })], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_cell"
      },
      "outputs": [],
      "source": [
        "# Visualize confidence distribution for additional test subjects\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.barh(additional_results_df['Product'], additional_results_df['Confidence'])\n",
        "plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Threshold ({best_threshold})')\n",
        "plt.xlabel('Confidence of being a Bell Pepper')\n",
        "plt.title('Confidence Scores for Additional Test Subjects')\n",
        "plt.xlim(0, 1)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_model_header"
      },
      "source": [
        "## 12. Save Model for Future Use\n",
        "\n",
        "Save both the trained model and the optimal threshold for deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model_cell"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained model and the optimal threshold\n",
        "with open('bell_pepper_high_recall_model.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'model': best_model, \n",
        "        'optimal_threshold': best_threshold\n",
        "    }, f)\n",
        "\n",
        "print(f\"Model saved as 'bell_pepper_high_recall_model.pkl' with optimal threshold {best_threshold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "production_header"
      },
      "source": [
        "## 13. Function for Production Use\n",
        "\n",
        "Create a reusable function for classifying products in production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "production_function_cell"
      },
      "outputs": [],
      "source": [
        "def classify_bell_pepper(product_name, model_path='bell_pepper_high_recall_model.pkl'):\n",
        "    \"\"\"\n",
        "    Classify a product as bell pepper or not using the saved model\n",
        "    \n",
        "    Args:\n",
        "        product_name: Name of the product to classify\n",
        "        model_path: Path to the saved model file\n",
        "        \n",
        "    Returns:\n",
        "        True if the product is a bell pepper, False otherwise\n",
        "    \"\"\"\n",
        "    # Load the model and threshold\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model_data = pickle.load(f)\n",
        "    \n",
        "    model = model_data['model']\n",
        "    threshold = model_data['optimal_threshold']\n",
        "    \n",
        "    # Get prediction probability\n",
        "    probability = model.predict_proba([product_name])[0][1]\n",
        "    \n",
        "    # Apply threshold\n",
        "    return probability >= threshold, probability\n",
        "\n",
        "# Example of how to use the function\n",
        "print(\"\\nExample of using the production function:\")\n",
        "for product in [\"Fresh Red Bell Pepper\", \"Mezzetta Roasted Bell Peppers\"]:\n",
        "    is_bell_pepper, confidence = classify_bell_pepper(product)\n",
        "    print(f\"Product: {product}\")\n",
        "    print(f\"Is bell pepper: {is_bell_pepper}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_header"
      },
      "source": [
        "## 14. Conclusion and Next Steps\n",
        "\n",
        "### Key findings:\n",
        "1. Lowering the threshold significantly improves recall but reduces precision\n",
        "2. The model can achieve perfect or near-perfect recall with an appropriate threshold\n",
        "3. Previously problematic \"tri-color\" bell peppers are now correctly classified\n",
        "4. The enhanced features and feature union approach improves overall performance\n",
        "\n",
        "### Next steps:\n",
        "1. Continue collecting more labeled data, especially for edge cases\n",
        "2. Implement a continuous feedback loop to identify and correct remaining false negatives\n",
        "3. Consider implementing a two-stage classifier for very high precision needs\n",
        "4. Deploy the model with the optimal threshold for production use\n",
        "5. Implement monitoring to track model performance over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "json_new_data_header"
      },
      "source": [
        "## 15. JSON Format for New Test Data\n",
        "\n",
        "Here's the JSON format for adding our new test data to the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "json_new_data_cell"
      },
      "outputs": [],
      "source": [
        "new_data_json = [\n",
        "  {\n",
        "    \"name\": \"Burpee 'Sweet Candy Apple' Hybrid | Bell Pepper\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Dried Red and Green Bell Peppers Mix by It's Delish, 1 lb Bulk Bag, All Natural\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Dried Red and Green Bell Peppers Mix by It's Delish, 9 oz Large Jar, Chopped, Spice Seasoning\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"GranAroma Red Bell Pepper Powder, Sweet Pepper Flavor, Baking & Cooking (17 Ounce)\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Green BELL Peppers 1 Pound Bulk Bag-Heat Sealed to Maintain Freshness-Crushed & Dried Spice Seasoning\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Holiday Wedding Party Decoration Mini Bell Peppers Vegetables For Decoration Pepper Decoration 2/3/4PCS\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Howard Foods Howards Sweet Pepper Relish, 11 oz Jar\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Mezzetta Sweet Banana Peppers, 16 fl oz Jar\",\n",
        "    \"is_bell_pepper\": false\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"TAZAH Chili Peppers 22oz (624g) â Pickled Hot Yellow Cascabella Peppers\",\n",
        "    \"is_bell_pepper\": false\n",
        "  }\n",
        "]\n",
        "\n",
        "print(json.dumps(new_data_json, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper_script_header"
      },
      "source": [
        "## 16. Helper Script for Finding Missing Products\n",
        "\n",
        "A utility function to find products not yet in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper_script_cell"
      },
      "outputs": [],
      "source": [
        "def find_missing_products(input_products, existing_data_path='bell_peppers.json'):\n",
        "    \"\"\"Find product names that don't exist in the existing data file\n",
        "    \n",
        "    Args:\n",
        "        input_products: List of input product names\n",
        "        existing_data_path: Path to the existing JSON data file\n",
        "        \n",
        "    Returns:\n",
        "        A list of missing product names\n",
        "    \"\"\"\n",
        "    # Load existing products\n",
        "    with open(existing_data_path, 'r') as f:\n",
        "        existing_data = json.load(f)\n",
        "    \n",
        "    # Extract just the product names\n",
        "    existing_product_names = {product[\"name\"] for product in existing_data}\n",
        "    \n",
        "    # Find missing products\n",
        "    missing_products = [product for product in input_products if product not in existing_product_names]\n",
        "    \n",
        "    return missing_products\n",
        "\n",
        "# Example usage\n",
        "example_products = [\n",
        "    \"Fresh Red Bell Pepper\",\n",
        "    \"Organic Green Bell Pepper\",\n",
        "    \"New Product Not In Dataset\",\n",
        "    \"Another New Product\"\n",
        "]\n",
        "\n",
        "missing = find_missing_products(example_products)\n",
        "print(f\"Missing products: {missing}\")\n",
        "\n",
        "# Function to append new data to existing dataset\n",
        "def append_to_dataset(new_products, is_bell_pepper_values, existing_data_path='bell_peppers.json', output_path=None):\n",
        "    \"\"\"Append new products to the existing dataset\n",
        "    \n",
        "    Args:\n",
        "        new_products: List of new product names\n",
        "        is_bell_pepper_values: List of boolean values indicating if each product is a bell pepper\n",
        "        existing_data_path: Path to the existing JSON data file\n",
        "        output_path: Path to save the updated dataset (if None, overwrites existing file)\n",
        "        \n",
        "    Returns:\n",
        "        The updated dataset\n",
        "    \"\"\"\n",
        "    # Load existing products\n",
        "    with open(existing_data_path, 'r') as f:\n",
        "        existing_data = json.load(f)\n",
        "    \n",
        "    # Create new entries\n",
        "    new_entries = [\n",
        "        {\"name\": name, \"is_bell_pepper\": is_bell_pepper}\n",
        "        for name, is_bell_pepper in zip(new_products, is_bell_pepper_values)\n",
        "    ]\n",
        "    \n",
        "    # Combine datasets\n",
        "    updated_data = existing_data + new_entries\n",
        "    \n",
        "    # Save if output path is provided\n",
        "    if output_path:\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(updated_data, f, indent=2)\n",
        "    \n",
        "    return updated_data\n",
        "\n",
        "# Example of how to use the append function\n",
        "# append_to_dataset(\n",
        "#     new_products=[\"New Product 1\", \"New Product 2\"],\n",
        "#     is_bell_pepper_values=[True, False],\n",
        "#     output_path=\"updated_bell_peppers.json\"\n",
        "# )"
      ]
    }
  ]
}
